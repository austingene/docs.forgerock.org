<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head>

      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Chapter&nbsp;19.&nbsp;Configuring OpenIDM to Work in a Cluster</title><link rel="stylesheet" type="text/css" href="coredoc.css"><link rel="stylesheet" type="text/css" href="css/coredoc.css"><link rel="stylesheet" type="text/css" href="sh/css/shCore.css"><link rel="stylesheet" type="text/css" href="sh/css/shCoreEclipse.css"><link rel="stylesheet" type="text/css" href="sh/css/shThemeEclipse.css"><script src="http://code.jquery.com/jquery-1.11.0.min.js" type="text/javascript"></script><script src="uses-jquery.js" type="text/javascript"></script><script src="sh/js/shCore.js" type="text/javascript"></script><script src="sh/js/shBrushAci.js" type="text/javascript"></script><script src="sh/js/shBrushBash.js" type="text/javascript"></script><script src="sh/js/shBrushCsv.js" type="text/javascript"></script><script src="sh/js/shBrushHttp.js" type="text/javascript"></script><script src="sh/js/shBrushJava.js" type="text/javascript"></script><script src="sh/js/shBrushJScript.js" type="text/javascript"></script><script src="sh/js/shBrushLDIF.js" type="text/javascript"></script><script src="sh/js/shBrushPlain.js" type="text/javascript"></script><script src="sh/js/shBrushProperties.js" type="text/javascript"></script><script src="sh/js/shBrushXml.js" type="text/javascript"></script><script src="sh/js/shAll.js" type="text/javascript"></script><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"><link rel="home" href="index.html" title="OpenIDM Integrator's Guide"><link rel="up" href="index.html" title="OpenIDM Integrator's Guide"><link rel="prev" href="chap-auditing.html" title="Chapter&nbsp;18.&nbsp;Using Audit Logs"><link rel="next" href="chap-mail.html" title="Chapter&nbsp;20.&nbsp;Sending Email"><link rel="copyright" href="legalnotice.html" title="Legal Notice"><script type="text/javascript">SyntaxHighlighter.all();</script>
<link rel="shortcut icon" href="http://forgerock.org/favicon.ico">
</head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Chapter&nbsp;19.&nbsp;Configuring OpenIDM to Work in a Cluster</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="chap-auditing.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="chap-mail.html">Next</a></td></tr></table><hr></div><div lang="en" class="chapter"><div class="titlepage"><div><div><h1 class="title"><a name="chap-cluster"></a>Chapter&nbsp;19.&nbsp;Configuring OpenIDM to Work in a Cluster</h1></div></div></div><div class="toc"><div class="toc-title">Table of Contents</div><dl class="toc"><dt><span class="section"><a href="chap-cluster.html#cluster-config">19.1. Configuring an OpenIDM Instance as Part of a Cluster</a></span></dt><dt><span class="section"><a href="chap-cluster.html#clustering-scheduled-tasks">19.2. Managing Scheduled Tasks Across a Cluster</a></span></dt><dt><span class="section"><a href="chap-cluster.html#cluster-over-REST">19.3. Managing Nodes Over REST</a></span></dt></dl></div><a class="indexterm" name="d0e20554"></a><a class="indexterm" name="d0e20557"></a><a class="indexterm" name="d0e20560"></a><p>
  To ensure availability of the identity management service, you can deploy
  multiple OpenIDM instances in a cluster. In a clustered environment, all
  instances point to the same external database. The database itself might or
  might not be clustered, depending on your particular availability strategy.
 </p><p>
  In a clustered environment, if one instance of OpenIDM becomes unavailable or
  does not check in with the cluster management service, another instance
  of OpenIDM detects this situation. If the unavailable instance did not
  complete one or more tasks, the available instance attempts to recover and
  rerun those tasks.
 </p><p>
  For example, if <code class="literal">instance1</code> goes down while executing a
  scheduled task, the cluster manager notifies the scheduler service that
  <code class="literal">instance1</code> is not available. The scheduler service then
  attempts to clean up any jobs that <code class="literal">instance1</code> was running
  when it went down.
 </p><p>
  This chapter describes what changes you need to make to OpenIDM to configure
  multiple instances that point to a database.
 </p><p>
  The following diagram depicts a relatively simple cluster configuration.
  You do need to do more than just set a unique value for
  <code class="literal">openidm.node.id</code>
 </p><div class="mediaobject" align="center"><a name="figure-cluster"></a><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="100%"><tr><td align="center"><img src="images/cluster-config.png" align="middle" height="360" alt="A Simplified Cluster"></td></tr></table><div class="longdesc-link" align="right"><br clear="all"><span class="longdesc-link">[<a href="figure-cluster.html" target="longdesc">D</a>]</span></div></div><p>
  The default installation of OpenIDM is pre-configured to enable the cluster
  service. The <code class="filename">conf/cluster.json</code> file includes the
  <code class="literal">"enabled" : true</code> directive. All you need to do with
  multiple instances of OpenIDM on the same subnet is modify each
  <code class="filename">boot.properties</code> file. Pay attention to the
  <code class="literal">openidm.node.id</code> and <code class="literal">openidm.instance.type</code>
  properties in that file.
 </p><p>
  When you configure a cluster, check the configuration files for each instance
  of OpenIDM. Except for <code class="filename">boot.properties</code>, the configuration
  files should be identical.
 </p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cluster-config"></a>19.1.&nbsp;Configuring an OpenIDM Instance as Part of a Cluster</h2></div></div></div><div class="toc"><dl class="toc"><dt><span class="section"><a href="chap-cluster.html#cluster-boot-config">19.1.1. Edit the Boot Configuration</a></span></dt><dt><span class="section"><a href="chap-cluster.html#cluster-config-file">19.1.2. Edit the Cluster Configuration</a></span></dt></dl></div><p>
   Before you configure an instance of OpenIDM to work in a cluster, make sure
   that OpenIDM is stopped. If someone had previously run that instance of
   OpenIDM, delete the <code class="filename">/path/to/openidm/felix-cache</code>
   directory.
  </p><p>
   All OpenIDM instances that form part of a single cluster must must all be
   configured to use the same type of repository.
   Note that OrientDB is currently unsupported in production environments.
  </p><p>
   To configure an individual OpenIDM instance as a part of a clustered
   deployment, follow these steps.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Configure OpenIDM for a MySQL repository, as described in <a href="../../install-guide/index/chap-repository.html" class="link"><em class="citetitle">Installing a
     Repository For Production</em></a> in the
     <em class="citetitle">Installation Guide</em>.
    </p><p>You need only import the data definition language script for OpenIDM
     into MySQL once, not repeatedly for each OpenIDM instance.
    </p></li><li class="listitem"><p><a class="xref" href="chap-cluster.html#cluster-boot-config" title="19.1.1.&nbsp;Edit the Boot Configuration">Section&nbsp;19.1.1, &#8220;Edit the Boot Configuration&#8221;</a></p></li><li class="listitem"><p><a class="xref" href="chap-cluster.html#cluster-config-file" title="19.1.2.&nbsp;Edit the Cluster Configuration">Section&nbsp;19.1.2, &#8220;Edit the Cluster Configuration&#8221;</a></p></li><li class="listitem"><p>
     If you are using scheduled tasks, do configure persistent schedules
     to ensure that they fire only once across the cluster. For more information,
     see the section on <a href="../../integrators-guide/index/chap-scheduler-conf.html#persistent-schedules" class="link"><em class="citetitle">Persisted
     Schedules</em></a>.
    </p></li></ol></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="cluster-boot-config"></a>19.1.1.&nbsp;Edit the Boot Configuration</h3></div></div></div><p>
    Each participating instance in a cluster must have its own unique node or
    instance ID, and must be attributed a role in the cluster. Specify these
    parameters in the <code class="filename">conf/boot/boot.properties</code> file of
    each instance.
   </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
      Specify a unique identifier for the instance, such as:
     </p><pre class="brush: plain;">
openidm.node.id=instance1
     </pre><p>
      On subsequent instances, the <code class="literal">openidm.node.id</code> can be set
      to <code class="literal">instance2</code>, <code class="literal">instance3</code>, and so
      forth. You can choose any value, as long as it is unique within the
      cluster.
     </p><p>
      In the cluster manager configuration file,
      <code class="filename">cluster.json</code>, the clustering service is enabled by
      default with the following setting:
     </p><pre class="brush: plain;">
"enabled": true
     </pre><p>
      The cluster manager specifies the OpenIDM instance ID
      from the <code class="filename">boot.properties</code> file as follows:
     </p><pre class="brush: plain;">
"instanceId" : "&amp;{openidm.node.id}",
     </pre><p>
      The scheduler uses the instance ID to claim and execute pending jobs. If
      multiple nodes have the same instance ID, problems will arise with
      multiple nodes attempting to execute the same scheduled jobs.
     </p><p>
      The cluster manager requires nodes to have unique IDs to ensure that it is
      able to detect when a node becomes unavailable.
     </p></li><li class="listitem"><p>
      Specify the instance type in the cluster.
     </p><p>
      On the primary instance, revise the following line in the
      <code class="filename">boot.properties</code> file as follows:
     </p><pre class="brush: plain;">
openidm.instance.type=clustered-first
     </pre><p>
      On subsequent instances, revise the following line in the
      <code class="filename">boot.properties</code> file as follows:
     </p><pre class="brush: plain;">
openidm.instance.type=clustered-additional
     </pre><p>
      The instance type is used during the setup process. When the primary node
      has been configured, additional nodes are bootstrapped with the security
      settings (keystore and truststore) of the primary node. After all nodes
      have been configured, they are all considered equal in the cluster, that
      is, there is no concept of a "master" node.
     </p><p>
      If no instance type is specified, the default value for this property is
      <code class="literal">openidm.instance.type=standalone</code>, which indicates that
      the instance will not be part of a cluster.
     </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="cluster-config-file"></a>19.1.2.&nbsp;Edit the Cluster Configuration</h3></div></div></div><p>
    The cluster configuration file is
    <code class="filename">/path/to/openidm/conf/cluster.json</code>. To enable a
    cluster, you should not have to make changes to this file:
   </p><pre class="brush: javascript;">{
 "instanceId" : "&amp;{openidm.node.id}",
 "instanceTimeout" : "30000",
 "instanceRecoveryTimeout" : "30000",
 "instanceCheckInInterval" : "5000",
 "instanceCheckInOffset" : "0",
 "enabled" : true
 } </pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
      The <code class="literal">instanceId</code> is set to the value of
      <code class="literal">openidm.node.id</code>, as configured in the
      <code class="filename">conf/boot/boot.properties</code> file.
     </p></li><li class="listitem"><p>
      <code class="literal">instanceTimeout</code> specifies the length of time (in
      milliseconds) that an instance can be "down" before the instance is
      considered to be in recovery mode.
     </p><p>
      <span class="emphasis"><em>Recovery mode</em></span> implies that the
      <code class="literal">instanceTimeout</code> of an instance has expired, and that
      another instance of OpenIDM in the cluster has detected that event. That
      second instance of OpenIDM is now attempting to
      <span class="emphasis"><em>recover</em></span> the instance.  The logic behind
      the recovery mechanism differs, depending on the component within OpenIDM.
      The scheduler component has well-defined recovery logic, and attempts to
      move any jobs that had been acquired by the unavailable instance back into
      the pool of waiting jobs.
     </p></li><li class="listitem"><p>
      <code class="literal">instanceRecoveryTimeout</code> specifies the length of time
      (in milliseconds) that an instance can be in recovery mode before that
      instance is considered to be offline.
     </p><p>
      The purpose of the recovery timeout is to prevent an instance from
      attempting to recover an unavailable instance indefinitely.
     </p></li><li class="listitem"><p>
      <code class="literal">instanceCheckInInterval</code> specifies the frequency (in
      milliseconds) that this instance checks in with the cluster manager to
      indicate that it is still online.
     </p></li><li class="listitem"><p>
      <code class="literal">instanceCheckInOffset</code> specifies an offset (in
      milliseconds) for the checkin timing, per instance, when a number of
      instances in a cluster are started simultaneously.
     </p><p>
      Specifying a checkin offset prevents a situation in which all the instances
      in a cluster check in at the same time, and place a strain on the cluster
      manager resource.
     </p></li><li class="listitem"><p>
      <code class="literal">enabled</code> notes whether or not the clustering service
      should be enabled when you start OpenIDM.
     </p></li></ul></div><p>
    If the default cluster configuration is not suitable for your deployment,
    edit the <code class="filename">cluster.json</code> file for each instance.
   </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="clustering-scheduled-tasks"></a>19.2.&nbsp;Managing Scheduled Tasks Across a Cluster</h2></div></div></div><div class="itemizedlist"><p>
    In a clustered environment, the scheduler service looks for pending jobs and
    handles them as follows:
   </p><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
     Non-persistent (in-memory) jobs will fire on each node in the cluster.
    </p></li><li class="listitem"><p>
     Persistent scheduled jobs are picked up and executed by a single node in
     the cluster.
    </p></li><li class="listitem"><p>
     Jobs that are configured as persistent but <span class="emphasis"><em>not concurrent</em></span>
     will fire only once across the cluster and will not fire again at the
     scheduled time, on the same node, or on a different node, until the current
     job has completed.
    </p><p>
     For example, a reconciliation operation that runs for longer than the time
     between scheduled intervals will not trigger a duplicate job while it is
     still running.
    </p></li></ul></div><p>
   The order in which nodes in a cluster claim jobs is random. If a node goes
   down, the cluster manager will automatically fail over jobs that have been
   claimed by that node, but not yet started. For example, if node A claims a
   job but does not start it, and then goes down, node B can reclaim that job.
   If node A claims and job, starts it, and then goes down, the job cannot be
   reclaimed by another node in the cluster. That specific job will never be
   completed. Instance B can claim the next iteration (or scheduled occurrence)
   of the job.
  </p><p>
   Note that this failover behavior is different to the behavior in OpenIDM
   2.1.0, in which an unavailable node would need to come up again to free a job
   that it had already claimed.
  </p><p>
   If a number of changes are made as a result of a LiveSync action, a single
   instance will claim the action, and will process all the changes related to
   that action.
  </p><p>
   To prevent a specific instance from claiming pending jobs,
   <code class="literal">"executePersistentSchedules"</code> should be set to
   <code class="literal">false</code> in the scheduler configuration for that instance.
   Because all nodes in a cluster read their configuration from a single
   repository you must use token substitution, via the
   <code class="filename">boot.properties</code> file, to define a specific scheduler
   configuration for each node.
  </p><p>
   So, if you want certain nodes to participate in processing clustered
   schedules (such as LiveSync) and other nodes not to participate, you can
   specify this information in the <code class="filename">conf/boot/boot.properties</code>
   file of each node. For example, to prevent a node from participating, add the
   following line to the <code class="filename">boot.properties</code> file of that node:
  </p><pre class="brush: plain;">
execute.clustered.schedules=false
  </pre><p>
   The initial scheduler configuration that is loaded into the repository must
   point to the relevant property in <code class="filename">boot.properties</code>. So,
   the initial <code class="filename">scheduler.json</code> file would include a token
   such as the following:
  </p><pre class="brush: plain;">
{
    "executePersistentSchedules" : "&amp;{execute.clustered.schedules}",
}
  </pre><p>
   You do not want to allow changes to a configuration file to overwrite the
   global configuration in the repository. To do so, start each instance of
   OpenIDM and then disable the file-based
   configuration view in a clustered deployment. For more information, see
   <span class="olink"><em class="citetitle">Disabling the
    File-Based Configuration View</em></span>.
  </p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cluster-over-REST"></a>19.3.&nbsp;Managing Nodes Over REST</h2></div></div></div><p>
   You can manage clusters and individual nodes over the REST interface, at the
   URL <code class="literal">https://localhost:8443/openidm/cluster/</code>. The following
   sample REST commands demonstrate the cluster information that is available
   over REST.
  </p><div class="example"><a name="d0e20864"></a><div class="example-title">Example&nbsp;19.1.&nbsp;Displaying the Nodes in the Cluster</div><div class="example-contents"><p>
    The following REST request displays the nodes configured in the cluster, and
    their status.
   </p><div class="screen"><pre>$ <strong>curl \
 --cacert self-signed.crt \
 --header "X-OpenIDM-Username: openidm-admin" \
 --header "X-OpenIDM-Password: openidm-admin" \
 --request GET \
 "https://localhost:8443/openidm/cluster" </strong>
    <em>
{
  "results": [
    {
      "shutdown": "",
      "startup": "2013-10-28T11:48:29.026+02:00",
      "instanceId": "instance1",
      "state": "running"
    },
    {
      "shutdown": "",
      "startup": "2013-10-28T11:51:31.639+02:00",
      "instanceId": "instance2",
      "state": "running"
    }
  ]</em>
}  </pre></div></div></div><br class="example-break"><div class="example"><a name="d0e20877"></a><div class="example-title">Example&nbsp;19.2.&nbsp;Checking the State of an Individual Node</div><div class="example-contents"><p>
    To check the status of a specific node, include its instance ID in the URL,
    for example:
   </p><div class="screen"><pre>$ <strong> curl \
 --cacert self-signed.crt \
 --header "X-OpenIDM-Username: openidm-admin" \
 --header "X-OpenIDM-Password: openidm-admin" \
 --request GET \
 "https://localhost:8443/openidm/cluster/instance1"</strong>
    <em>
{
  "results": {
    "shutdown": "",
    "startup": "2013-10-28T11:48:29.026+02:00",
    "instanceId": "instance1",
    "state": "running"
  }</em>
}  </pre></div></div></div><br class="example-break"></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="chap-auditing.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="chap-mail.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Chapter&nbsp;18.&nbsp;Using Audit Logs&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;Chapter&nbsp;20.&nbsp;Sending Email</td></tr></table></div><p>&nbsp;</p><div id="footer"><p>Something wrong on this page? <a href="https://bugster.forgerock.org/jira/secure/CreateIssueDetails!init.jspa?pid=10020&components=10164&issuetype=1">Log a documentation bug.</a></p></div>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-23412190-9']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</body></html>